"ประเทศไทยมีข้อมูลกฎหมายและประกาศหลายล้านฉบับ แต่ทำไมเราถึงยังไม่มี AI กฎหมายเก่งๆ หรือระบบค้นหาที่เข้าใจบริบทได้สักที?"

คำตอบสั้นๆ คือ "ข้อมูล" ครับ... ข้อมูลส่วนใหญ่ของเรายังติดอยู่ในรูปแบบ PDF หรือรูปภาพสแกน (Unstructured Data) ที่มนุษย์อ่านรู้เรื่อง แต่คอมพิวเตอร์และ LLM กลับมองเห็นเป็นเพียงความว่างเปล่า

นี่คือจุดเริ่มต้นของ Open Law Data Thailand โครงการ Civic Tech ที่เกิดจากความร่วมมือระหว่าง ภาคประชาชน (Tech) และ ฝ่ายนิติบัญญัติ (สว.) โดยมีเป้าหมายเดียวคือการ "ทลายกำแพง PDF" เพื่อเปลี่ยนเอกสารประวัติศาสตร์ของชาติ ให้กลายเป็นโครงสร้างพื้นฐานทางข้อมูล (Data Infrastructure) ที่นักพัฒนาสามารถนำไปใช้งานได้จริง

<!--break-->

บทความนี้จะพาไปเจาะลึก Behind the Scenes ตั้งแต่ Day 1

จากไอเดียสู่การตั้งคณะทำงานร่วมกับ สว. และการขอความร่วมมือจากสำนักเลขาธิการคณะรัฐมนตรี (สลค.) ผู้เป็นพันธมิตรรายแรกของโครงการ

การออกแบบสถาปัตยกรรม "Zero Budget" ที่รัน Data Pipeline บน Synology NAS ฝากไฟล์ระดับร้อย Gigabyte ไว้บน Hugging Face

การแก้ปัญหาทางวิศวกรรม: รับมือไฟล์ 1.3 ล้านฉบับ, แก้บั๊ก Time Zone, และการใช้งาน API OCR ภาษาไทย

จนถึงวันที่เราเปิด Hugging Face Repository ให้ทุกคนเข้าถึงข้อมูลกฎหมายไทยได้ฟรี

ไม่ว่าคุณจะเป็น Data Engineer, Developer สาย NLP หรือคนที่สนใจเรื่อง Open Data นี่คือบันทึกการเดินทางของการเปลี่ยน "กองกระดาษ" ให้เป็น "ขุมทรัพย์ดิจิทัล" ครับ

**สำหรับใครที่อยากลองเล่นข้อมูล หรืออยากมาช่วยกันแก้บั๊ก เข้ามาคุยกันใน Discord ได้เลยครับ (ลิงก์อยู่ท้ายบทความ)**

## บทที่ 1: เมื่อภาคธุรกิจทางตัน และนักพัฒนาไม่มีทางไป

#### โจทย์จากภาคธุรกิจ: "ความเสี่ยงที่มองไม่เห็น"

จุดเริ่มต้นของเรื่องนี้ ไม่ได้เริ่มจากห้องทดลองทางคอมพิวเตอร์ แต่เริ่มจาก **"ความเดือดร้อนของผู้ประกอบการ"**

ท่านสมาชิกวุฒิสภา (สว.) ซึ่งมาจากสายอุตสาหกรรม ได้สัมผัสปัญหาจริงที่ผู้ประกอบการไทยต้องเจอทุกวัน นั่นคือ **"การเข้าถึงกฎหมายเป็นเรื่องยากและต้นทุนสูง"**

* กฎหมายและระเบียบข้อบังคับต่างๆ กระจัดกระจายอยู่ตามเว็บราชการนับร้อยแห่ง
* ผู้ประกอบการรายย่อย (SME) หรือนักลงทุนต่างชาติ ไม่รู้ว่าจะไปหาข้อมูลที่ถูกต้องและเป็นปัจจุบันได้จากที่ไหน
* ความไม่รู้นี้กลายเป็น **"ความเสี่ยง"** และเป็นอุปสรรคในการทำธุรกิจ

ท่าน สว. จึงมีวิสัยทัศน์ว่า **"เราต้องทำระบบฐานข้อมูลกฎหมายที่ค้นหาง่าย รวดเร็ว และแม่นยำ"** เพื่อปลดล็อกปัญหานี้ให้ภาคธุรกิจเดินหน้าได้


#### ความจริงจากฝั่ง Tech: "เราไม่มีกุญแจไขข้อมูล"

เมื่อท่าน สว. นำโจทย์นี้มาคุยกับผมซึ่งทำงานสาย Tech คำตอบทางเทคนิคคือ "ทำได้ครับ ระบบค้นหาหรือ AI ไม่ใช่เรื่องยาก... **แต่ปัญหาอยู่ที่ข้อมูล**"

เราต้องชี้แจงความจริงที่น่าเจ็บปวดให้ท่านฟังว่า ประเทศไทย **"ประเทศไทย ไม่มีฐานข้อมูลกฎหมายที่นำมาประมวลผลได้จริง"**

* **ความจริงคือ:** กฎหมายเป็นข้อมูลเปิดเผย (Open Information) ใครก็กดดูผ่านเว็บได้
* **แต่ปัญหาคือ:** รัฐไม่มีช่องทางให้นักพัฒนา "เชื่อมต่อ" หรือ "ดึงข้อมูล" ไปใช้งานได้เลย (No Open API / No Developer Access / No Open Data)

เหมือนเรามีห้องสมุดขนาดใหญ่ที่เปิดไฟสว่างไสว แต่ห้ามยืมหนังสือออกไป และห้ามถ่ายเอกสาร ต้องนั่งจดด้วยมือทีละหน้าเท่านั้น — **นักพัฒนาจึงไม่สามารถนำข้อมูลกฎหมายไปสร้าง Application หรือเทรน AI เพื่อแก้ปัญหาให้ผู้ประกอบการตามโจทย์ของท่าน สว. ได้**

#### ทางออก: การจับมือของ "อำนาจรัฐ" และ "อาวุธทางเทคนิค"

เมื่อเห็นปัญหาตรงกันว่า **"มีของ (ข้อมูล) แต่เอาออกมาใช้ไม่ได้"** จึงนำไปสู่การตั้ง **"คณะทำงาน Open Law Data"**

เราตกลงกันว่า จะไม่ใช้วิธีการไปกดดันให้หน่วยงานรัฐต้องลงทุนสร้างระบบ IT ใหม่ราคาแพง (เพราะอาจไม่ทันการและไม่ใช่ความเชี่ยวชาญของเขา) แต่เราจะเข้าไปในฐานะ **"ผู้ช่วยเคลียร์ทาง"**

* **บทบาทท่าน สว.:** ใช้กลไกนิติบัญญัติ ประสานงานกับหน่วยงานเจ้าของข้อมูล ขอความร่วมมือให้หน่วยงานภาครัฐส่งมอบไฟล์ดิจิทัล หรือ API อนุญาตให้เราเข้าถึงข้อมูลดิบจำนวนมหาศาลได้
* **บทบาททีม Tech:** รับข้อมูลเหล่านั้นมา แล้วทำหน้าที่เป็น **"โรงงานแปรรูป"** — จัดการ Clean Data, ทำ OCR, และแปลงโครงสร้าง ให้กลายเป็น Dataset มาตรฐาน

#### ภารกิจร่วมกัน

เป้าหมายของบทที่ 1 นี้จึงจบลงที่ข้อตกลงร่วมกันว่า: **"เราจะทำหน้าที่อำนวยความสะดวกให้ภาครัฐ เพื่อดึงข้อมูลกฎหมายที่กระจัดกระจาย มารวมศูนย์และแปลงสภาพ แล้วเปิดให้นักพัฒนาและภาคประชาชนดาวน์โหลดไปใช้งานได้จริงๆ เสียที"**

เพื่อที่สุดท้ายแล้ว ผู้ประกอบการจะเข้าถึงกฎหมายได้ง่ายขึ้น และนักพัฒนาก็มีข้อมูลไปสร้างนวัตกรรมตามที่ฝันไว้


![เมื่อภาคธุรกิจทางตัน และนักพัฒนาไม่มีทางไป](https://www.openlawdatathailand.org/press/2025-12-31_blognone/1.jpg)

## บทที่ 2: พันธมิตรแรกและจุดเริ่มต้นของโครงการ

#### ตามหาคนที่ "ใจตรงกัน"

หลังจากตั้งคณะทำงาน เป้าหมายแรกของเราคือการหา "Data Source" ที่สำคัญที่สุดของประเทศ นั่นคือ **"ราชกิจจานุเบกษา"**

เราได้มีโอกาสเข้าไปหารือกับ **"สำนักเลขาธิการนายกรัฐมนตรี"** ซึ่งเป็นหน่วยงานหลักในการดูแลประกาศราชกิจจานุเบกษา ในตอนแรกพวกเราเตรียมตัวไปนำเสนอโครงการอย่างเต็มรูปแบบ

แต่เมื่อได้พูดคุยกับพี่ๆ ข้าราชการที่สำนักเลขาฯ เรากลับพบสิ่งที่น่าประทับใจมาก คือ **"ใจพวกท่านมาทางเดียวกับเราอยู่แล้ว"** พี่ๆ ที่ดูแลข้อมูลมีความตั้งใจอยากเห็นข้อมูลเหล่านี้ถูกนำไปใช้ประโยชน์ อยากให้ประชาชนเข้าถึงง่าย และมีความเข้าใจในเทคโนโลยีเป็นทุนเดิม

จึงกลายเป็น **"การจับมือร่วมงาน"** ที่ราบรื่นอย่างเหลือเชื่อ เพราะต่างฝ่ายต่างมีเป้าหมายเดียวกัน คือทำเพื่อประโยชน์ของนักพัฒนาและประชาชน

#### ราชกิจจานุเบกษา: จิ๊กซอว์ชิ้นแรก

ความสำเร็จแรกที่ยิ่งใหญ่ที่สุดของโครงการ คือการได้รับความอนุเคราะห์ข้อมูล **"ราชกิจจานุเบกษา"** มาเป็น Dataset ชุดแรก

นี่ไม่ใช่แค่ไฟล์เอกสารธรรมดา แต่มันคือกฎหมาย ประกาศ และคำสั่ง ที่ครอบคลุมประวัติศาสตร์และทิศทางของประเทศไทย การได้ข้อมูลชุดนี้มาเข้าสู่ Data Pipeline เปรียบเสมือนการวางศิลาฤกษ์ที่ทำให้โครงการ Open Law Data Thailand เกิดขึ้นได้จริง

#### Win-Win Solution: เมื่อเราช่วย "แบก" แทนรัฐ

นอกจากการได้ข้อมูลแล้ว โมเดลความร่วมมือนี้ยังสร้างประโยชน์ในเชิง Technical ให้กับภาครัฐอย่างมหาศาล ซึ่งเป็นจุดขายสำคัญที่เราค้นพบ

ในอดีต เมื่อนักพัฒนาต้องการข้อมูลราชกิจจาฯ จำนวนมาก วิธีเดียวที่ทำได้คือการเขียนโปรแกรม **"Scraping"** (ดูดข้อมูล) ยิงเข้าไปที่เว็บไซต์ของหน่วยงานรัฐรัวๆ ซึ่งส่งผลเสียร้ายแรง:

1. **Server Load:** ทำให้เว็บไซต์ราชการทำงานหนัก รองรับผู้ใช้งานทั่วไปไม่ไหว หรือถึงขั้นเว็บล่ม
2. **Infrastructure Cost:** ภาครัฐต้องเสียงบประมาณมหาศาลในการขยาย Server เพื่อรองรับ Traffic เหล่านี้

เราจึงเสนอตัวเข้ามาเป็น **"ตัวกลาง" (Buffer)**

* **ทางหน่วยงานรัฐ:** ส่งข้อมูลดิบมาให้เรา
* **ทางเรา:** นำข้อมูลมา Hosting ไว้บน Platform ที่รองรับโหลดได้สูง (เช่น Hugging Face)

**ผลลัพธ์คือ:**

* **นักพัฒนา:** ได้โหลดข้อมูลแบบ Clean Data เต็มก้อนไปใช้ได้ทันที ไม่ต้องเสี่ยงเขียน Scraper ให้โดนบล็อก
* **ภาครัฐ:** **ประหยัดงบประมาณ** ไม่ต้องตั้ง Server ราคาแพงเพื่อรองรับบอท และเว็บไซต์หลักก็ทำงานได้ลื่นไหลเพื่อให้บริการประชาชนทั่วไป

ความร่วมมือกับสำนักเลขาธิการนายกรัฐมนตรีในครั้งนี้ จึงเป็น **Use Case ต้นแบบ** ที่พิสูจน์ให้เห็นว่า การทำ Open Data ไม่ใช่ภาระ แต่เป็นการ "ช่วยกันแบก" ที่ทำให้ทุกฝ่ายเบาแรงลงและได้ประโยชน์สูงสุด

และนี่คือจุดเริ่มต้นที่ทำให้เรามั่นใจว่า... เรามาถูกทางแล้ว

![พันธมิตรแรกและจุดเริ่มต้น](https://www.openlawdatathailand.org/press/2025-12-31_blognone/2.jpg)


## บทที่ 3: สถาปัตยกรรมแบบ "Zero Budget"

เมื่อเราได้ข้อมูลดิบมาแล้ว โจทย์ใหญ่ถัดมาคือ "จะเอามันไปเก็บไว้ที่ไหน?" และ "จะใช้อะไรประมวลผล?"

ในโครงการระดับชาติปกติ เราอาจจะนึกถึง Cloud Server ราคาแพงหรือ Data Center ขนาดใหญ่ แต่สำหรับ **Open Law Data Thailand** ซึ่งขับเคลื่อนด้วยใจและงานอาสาสมัคร และเราต้องประหยัดค่าใช้จ่ายให้มากที่สุด

ดังนั้น Requirement ของสถาปัตยกรรมระบบเราจึงไม่ใช่แค่ "ทำงานได้" แต่ต้อง "คุ้มค่า" และ "ยั่งยืน" เราจึงต้องเฟ้นหา Solution ที่ฉลาดที่สุดมาแก้ปัญหานี้


#### ด่านแรก: บ้านหลังเก่าที่ GitHub และกำแพง Quota

เริ่มแรก เรามองไปที่ **GitHub** ซึ่งเป็นบ้านสามัญประจำใจของนักพัฒนาทั่วโลก เราตั้งใจจะฝากไฟล์ JSON และ Code ทั้งหมดไว้ที่นั่น เพื่อให้ Dev คนอื่นเข้าถึงง่ายที่สุด

แต่เมื่อเราเริ่ม Run Pipeline ไปสักพัก ข้อมูลราชกิจจานุเบกษาและบันทึกการประชุมที่มีจำนวนมหาศาล ก็เริ่มส่งสัญญาณเตือน



* **Storage Limit:** พื้นที่เริ่มเต็ม โดยเฉพาะเมื่อเราต้องเก็บไฟล์ประวัติศาสตร์ย้อนหลังนับร้อยปี
* **Bandwidth Quota:** GitHub มีข้อจำกัดเรื่อง Git LFS (Large File Storage) Bandwidth หากมีคนมาดาวน์โหลด Dataset ของเราพร้อมกันเยอะๆ (ซึ่งเป็นเป้าหมายของเรา) ระบบจะชนเพดาน Quota ทันที และอาจถูกระงับการใช้งาน

เราค้นพบว่า GitHub เหมาะกับการเก็บ Code แต่ไม่ได้ถูกออกแบบมาให้เป็น "โกดัง Data ขนาดใหญ่" สำหรับ Public Access แบบฟรีๆ


#### การอพยพครั้งใหญ่สู่ Hugging Face

เราจึงต้องมองหาบ้านหลังใหม่ และเราก็พบกับ **Hugging Face**

Hugging Face เป็นเหมือนสวรรค์ของคนทำ AI ในยุคนี้ ข้อดีที่ตอบโจทย์เราเป๊ะๆ คือ:

1. **Designed for Datasets:** มันถูกสร้างมาเพื่อเก็บ Dataset ระดับ Terabyte สำหรับเทรน AI โดยเฉพาะ เรื่อง Quota จึงยืดหยุ่นกว่า GitHub มาก
2. **Dataset Viewer:** มีระบบพรีวิวข้อมูลในหน้าเว็บได้เลย คนทั่วไปกดดูได้โดยไม่ต้องโหลด
3. **Community-Centric:** เป็นแหล่งรวมตัวของ Data Scientist อยู่แล้ว (ตรงกลุ่มเป้าหมาย)

ที่สำคัญที่สุดคือ **"ฟรี"** สำหรับ Public Dataset เราจึงตัดสินใจย้ายฐานข้อมูลทั้งหมดไปที่ Hugging Face ซึ่งกลายเป็นจุดเปลี่ยนสำคัญที่ทำให้เรากล้าเปิดรับข้อมูลจำนวนมหาศาลได้อย่างสบายใจ

#### ขุมพลังหลังบ้าน: ทำไมต้องใช้ NAS

เมื่อมีที่เก็บของแล้ว (Storage) คำถามต่อมาคือ "ใครจะเป็นคนทำงาน?" (Compute)

Data Pipeline ของเราต้องตื่นมาทำงานทุกวัน: *ตรวจสอบข้อมูลใหม่บนราชกิจจาฯ -> โหลดไฟล์ -> รัน OCR -> แปลงข้อมูล -> อัปโหลดขึ้น Hugging Face*

ถ้าเราไปเช่า Cloud Server เพื่อเปิดทิ้งไว้ 24 ชั่วโมง หรือรันงานหนักๆ ค่าใช้จ่ายรายเดือนจะสูงมากจนแบกรับไม่ไหว

คำตอบของเราจึงกลับมาสู่ความเรียบง่ายและประหยัด นั่นคือ **"Synology NAS"**

* **Low Cost:** เราใช้ NAS ลงทุนครั้งเดียวจบ ไม่ต้องจ่ายรายเดือน
* **Energy Efficient:** กินไฟน้อยมากเมื่อเทียบกับการเปิด PC ทิ้งไว้
* **Docker Support:** เราสามารถเขียน Script เป็น Docker Container แล้วโยนไปรันบน NAS ได้เลย ให้มันทำงานเงียบๆ อยู่มุมห้อง ค่อยๆ ประมวลผลข้อมูลวันละนิดละหน่อยอย่างสม่ำเสมอ

#### บทสรุปของสถาปัตยกรรม

ในที่สุด เราก็ได้โมเดลสถาปัตยกรรมที่ลงตัวและยั่งยืน:

* **Processing Unit:** รันบน **Synology NAS** ต้นทุนต่ำ มีความเสถียร
* **Storage & Distribution:** ฝากไว้ที่ **Hugging Face** (ฟรี, รองรับโหลดมหาศาล, เข้าถึงง่าย)

นี่คือโมเดล **"Civic Tech Architecture"** ที่พิสูจน์แล้วว่า การทำโครงการ Data ระดับประเทศ ไม่จำเป็นต้องใช้งบประมาณมาก แต่ใช้ความเข้าใจในเครื่องมือและการบริหารจัดการทรัพยากรที่มีอยู่อย่างชาญฉลาด

![สถาปัตยกรรมแบบ "Zero Budget](https://www.openlawdatathailand.org/press/2025-12-31_blognone/3.jpg)

## บทที่ 4: วิกฤต 1.3 ล้านไฟล์ และบทเรียนราคาแพงบน Git

#### ฝันร้ายของข้อมูลมหาศาล (The Nightmare of Scale)

หลังจากที่เราดีใจได้ไม่นานที่ได้รับข้อมูลราชกิจจานุเบกษาย้อนหลังตั้งแต่ปี พ.ศ. 2428 จนถึงปัจจุบัน ความจริงที่น่าขนลุกก็ปรากฏขึ้นตรงหน้า

เราไม่ได้กำลังจัดการกับเอกสารหลักพัน หรือหลักหมื่นฉบับ แต่เรากำลังเจอกับไฟล์ PDF จำนวนกว่า **1.3 ล้านฉบับ** ที่มีขนาดรวมกันระดับ **170GB**

ด้วยความที่ทีมงานคุ้นเคยกับ Git (Version Control System) เราจึงคิดง่ายๆ ว่า *"ก็แค่ Commit และ Push ขึ้นไปทั้งหมดเลยสิ"* ... และนั่นคือจุดเริ่มต้นของหายนะทางเทคนิค


#### เมื่อ Git "สำลัก" ข้อมูล

ทันทีที่เราพยายามยัดไฟล์ 1.3 ล้านชิ้นเข้าไปใน Repository เดียว ระบบก็เริ่มส่งสัญญาณล่มสลาย:

1. **Index Failure:** Git ไม่ได้ถูกออกแบบมาให้จัดการกับ File count หลักล้านใน Folder เดียว Index ของ Git บวมเป่งจนเครื่องคอมพิวเตอร์ที่ใช้รัน Pipeline ค้างไปดื้อๆ
2. **Unclonable Repo:** ลองจินตนาการถึงนักพัฒนาทั่วไปที่อยากได้ข้อมูลไปลองเล่น เขาต้องสั่ง `git clone` แล้วรอดาวน์โหลดไฟล์ระดับร้อย GB นานหลายวันกว่าจะเสร็จ... ซึ่งในความเป็นจริง ไม่มีใครทำได้ และไม่มีใครอยากทำ
3. **Hugging Face Reach Limit:** แม้แต่ Server ของ Hugging Face เอง ก็แจ้งว่าเราใช้ถึง Limit แล้ว เมื่อเจอกับปริมาณ Request มหาศาลขนาดนี้

เรามาถึงทางตัน... เรามีข้อมูลที่มีค่าที่สุดอยู่ในมือ แต่เราไม่มีปัญญาแจกจ่ายมันออกไป เพราะ "ท่อ" ที่เราเลือกใช้ มันเล็กเกินกว่าจะรองรับ "เขื่อน" ที่แตกทะลักออกมา


#### รื้อระบบใหม่: สถาปัตยกรรม Hot/Cold Storage

เราต้องถอยกลับมาตั้งหลักใหม่ และเปลี่ยนวิธีคิดจาก "File Storage" เป็น **"Data Archive"** โดยนำแนวคิด **Hot/Cold Storage** มาประยุกต์ใช้กับ Git

เราแบ่งข้อมูลออกเป็น 2 โซน เพื่อแก้ปัญหาเรื่อง Performance และ User Experience:


##### 1. Cold Data: แช่แข็งอดีต (The Archive)

* **คืออะไร:** ข้อมูลเก่าที่มีอายุมากกว่า 3 เดือนขึ้นไป (ย้อนหลังไปถึง พ.ศ.2428)
* **ปัญหาเดิม:** ไฟล์ PDF เก่า 1.3 ล้านไฟล์ คือตัวการที่ทำให้ Git พัง
* **ทางแก้:** เราจับไฟล์เหล่านี้มา **"มัดรวม" (Batching)** เป็นรายเดือน แล้วบีบอัดเป็นไฟล์ `.zip` แล้วใช้งานผ่านฟีเจอร์ Git LFS
    * เช่น `1999-01.zip` ภายในบรรจุ PDF ของเดือนมกราคม ปี 2542 ทั้งหมด
* **ผลลัพธ์:** จากเดิมที่มีไฟล์ 1.3 ล้านชิ้นในระบบ ลดฮวบลงเหลือเพียงแค่ **หลักพันชิ้น** (ตามจำนวนเดือน) เท่านั้น! Git กลับมาทำงานได้ลื่นไหล และนักพัฒนาสามารถเลือกโหลดเฉพาะช่วงปีที่ต้องการได้ (เช่น โหลดแค่ปี 2540 ไปวิเคราะห์วิกฤตต้มยำกุ้ง) โดยไม่ต้องโหลดทั้งหมด


##### 2. Hot Data: ข้อมูลสดใหม่ (The Fresh Zone)

* **คืออะไร:** ข้อมูลราชกิจจาฯ ล่าสุด ในรอบ 3 เดือนย้อนหลัง
* **ทางแก้:** ส่วนนี้เรายังคงเก็บเป็น **ไฟล์ PDF แยกรายฉบับ** เหมือนเดิม ไม่ Zip
* **เหตุผล:** เพื่อให้ระบบอัตโนมัติ (Bot) หรือนักข่าว สามารถเข้ามาดูประกาศรายวันได้ทันทีแบบ Real-time โดยไม่ต้องเสียเวลาแตกไฟล์ Zip และเพื่อให้ง่ายต่อการตรวจสอบความถูกต้องของ Pipeline รายวัน


#### จัดระเบียบชั้นวาง (Structuring the Library)

นอกจากเรื่อง Zip แล้ว เรายังจัดโครงสร้าง Folder ใหม่แบบ **Hierarchical Structure** เพื่อไม่ให้ Folder บวม (Directory Bloat):


```
YYYY / YYYY-MM / ไฟล์เอกสาร
```

การทำแบบนี้ช่วยให้ระบบ File System ค้นหาไฟล์ได้เร็วขึ้น และดูเป็นระเบียบเรียบร้อยสำหรับมนุษย์ที่กดเข้ามาดูผ่านหน้าเว็บ

#### ผลลัพธ์ของการออกแบบใหม่

การเปลี่ยนสถาปัตยกรรมครั้งนี้คือ "Game Changer"

* **Git รอดตาย:** ระบบสามารถจัดการข้อมูล 1.3 ล้านฉบับได้โดยไม่กิน Resource มหาศาล
* **User Friendly:** นักพัฒนาแฮปปี้มาก เพราะสามารถเลือกจิ้มโหลด `2024-01.zip` ไปเทรนโมเดลได้เลยในขนาดไม่กี่ GB ไม่ต้อง Clone ทั้งจักรวาลมาลงเครื่อง
* **Sustainability:** โครงสร้างนี้รองรับข้อมูลที่จะเพิ่มขึ้นในอนาคตได้อีก 100 ปี โดยที่ระบบไม่ช้าลง

บทเรียนจากบทนี้สอนให้เรารู้ว่า... **"การมีข้อมูลเยอะเป็นเรื่องดี แต่การออกแบบวิธีส่งมอบข้อมูล (Delivery) สำคัญยิ่งกว่า"**

![วิกฤต 1.3 ล้านไฟล์ และบทเรียนราคาแพงบน Git](https://www.openlawdatathailand.org/press/2025-12-31_blognone/4.jpg)

## บทที่ 5: ภารกิจสร้างท่อส่งข้อมูลและกำแพงที่มองไม่เห็น

#### เมื่อประตูหน้าปิดตาย

หลังจากวางสถาปัตยกรรม "NAS + Hugging Face" เสร็จสิ้น ก็ถึงเวลาที่เรารอคอย คือการเดินเครื่องจักรเพื่อดูดข้อมูล (Fetching) เข้าสู่ระบบ

เราเริ่มยิง Request แรกด้วยความมั่นใจเพื่อดึงข้อมูลย้อนหลังทั้งหมด... แต่แล้วเราก็ชนเข้ากับกำแพงที่มองไม่เห็น


##### อุปสรรคที่ 1: กับดักหน้า 450

**ปัญหา:** ระบบ API ของราชกิจจานุเบกษาที่เราได้รับอนุญาตให้เชื่อมต่อนั้น มีกลไกป้องกันตัวเองอยู่อย่างหนึ่ง คือ **"Pagination Limit"**

* เราสามารถกด "ถัดไป" ได้เรื่อยๆ แต่พอถึง **หน้า 450** (หรือประมาณ 9,000 รายการ) ระบบจะตัดจบดื้อๆ ไม่แสดงข้อมูลต่อ
* ปัญหานี้ร้ายแรงมาก เพราะข้อมูลราชกิจจาฯ มีเป็นล้านฉบับ การที่ API ตัดจบแค่ 9,000 รายการ แปลว่าเราจะไม่มีวันเข้าถึงข้อมูลเก่าๆ ที่อยู่ลึกกว่านั้นได้เลย หากใช้วิธีการดึงแบบปกติ (Sequential Fetching)

**วิธีแก้ไข: กลยุทธ์ "ซอยย่อยรายวัน"** เมื่อขอดึง "ทั้งหมด" ไม่ได้ เราจึงเปลี่ยนกลยุทธ์เป็นการขอดึง **"ทีละนิด"**

* ทีม Tech เขียน Python Script ขึ้นมาใหม่ โดยเปลี่ยน Logic จากการดึงตามลำดับ เป็นการดึงแบบ **"Day-by-Day Loop"**
* **วิธีการ:** สั่งให้บอทถาม API ว่า *"วันที่ 1 มกราคม 2428 มีประกาศอะไรบ้าง?"* -> เก็บข้อมูล -> *"วันที่ 2 มกราคม 2428 มีอะไรบ้าง?"* ... วนลูปไปเรื่อยๆ จนถึงปัจจุบัน
* **ผลลัพธ์:** แม้วันหนึ่งจะมีประกาศเยอะแค่ไหน ก็ไม่มีทางเกิน 9,000 รายการ ทำให้เราสามารถทะลุขีดจำกัดหน้า 450 ได้ และมั่นใจได้ 100% ว่าจะเก็บข้อมูลได้ครบทุกเม็ด ไม่มีตกหล่นแม้แต่วันเดียว


---


##### อุปสรรคที่ 2: การป้องกันจากต้นทาง

**ปัญหา:** เมื่อเราแก้เรื่อง Pagination ได้แล้ว เราก็เริ่มเร่งความเร็วในการดึงข้อมูล แต่สิ่งที่ตามมาคือ **"Connection & Rate Limiting"**

* การยิง Request ถี่ๆ ติดต่อกันเป็นเวลานาน ทำให้ Server ปลายทางมองว่าเราเป็นภัยคุกคาม (หรือแค่รับโหลดไม่ไหว) ส่งผลให้ Connection ถูกตัดขาด (Drop) หรือโดนบล็อกชั่วคราว
* ยิ่งไปกว่านั้น อินเทอร์เน็ตที่บ้าน (ที่ตั้ง NAS) ไม่ได้เสถียรระดับ Data Center พอรันงานข้ามคืนแล้วเน็ตตัด งานที่ทำมาทั้งหมดก็พัง ต้องมานั่งเริ่มนับหนึ่งใหม่

**วิธีแก้ไข: ความยืดหยุ่นและการรักษาตัวเอง (Resilience & Self-Healing)** เราต้องสอนให้บอทของเรามีความ "อดทน" และ "ฉลาด" มากขึ้น:

1. **ใช้ Private Proxy (NGINX):**
    * เราตั้ง NGINX ขึ้นมาเป็นด่านหน้า เพื่อบริหารจัดการ Connection คอยทำหน้าที่ Buffer ไม่ให้ยิง Request ออกไปดิบๆ จนล้นท่อ
2. **ระบบ Retry แบบ Exponential Backoff:**
    * เขียน Logic ให้บอทรู้จักการ "ถอย" เมื่อเจอปัญหา
    * ถ้า Server ปลายทางตอบ Error กลับมา แทนที่จะยิงซ้ำรัวๆ บอทจะหยุดรอ 1 วินาที... ถ้ายังไม่ได้ รอ 2 วินาที... 4 วินาที... 8 วินาที (คูณ 2 ไปเรื่อยๆ)
    * วิธีนี้ช่วยลดภาระ Server ปลายทาง และเพิ่มโอกาสสำเร็จในการเชื่อมต่อใหม่อย่างนุ่มนวล
3. **ระบบรักษาตัวเอง (Self-Healing & Checkpointing):**
    * ฟีเจอร์ที่สำคัญที่สุดคือ **"ความจำ"**
    * เราเขียนระบบให้บันทึกสถานะล่าสุด (Checkpoint) ลงไฟล์ตลอดเวลา ว่าดึงข้อมูลถึงวันที่เท่าไหร่แล้ว
    * หากไฟดับ เน็ตหลุด หรือโปรแกรม Crash เมื่อระบบกลับมาทำงานใหม่ มันจะอ่านค่า Checkpoint แล้ว **"ทำงานต่อจากจุดเดิมทันที"** ไม่ต้องเสียเวลาเริ่มใหม่จากศูนย์

#### บทสรุป: ท่อส่งข้อมูลที่เป็นอมตะ

ด้วยการผสมผสานเทคนิค Day-by-Day Loop และระบบ Self-Healing ทำให้ตอนนี้ Data Pipeline ของเราเปรียบเสมือน **"เครื่องจักรนิรันดร์"**

มันทำงานเงียบๆ อยู่บน NAS ตลอด 24 ชั่วโมง ค่อยๆ เก็บข้อมูลทีละวันๆ อย่างใจเย็น ไม่หวั่นแม้อินเทอร์เน็ตจะหลุด หรือ Server ปลายทางจะสะดุด ข้อมูลทุกตัวอักษรจากอดีตจนถึงปัจจุบัน กำลังถูกลำเลียงเข้าสู่คลังแสงของเราอย่างช้าๆ แต่มั่นคง


![ภารกิจสร้างท่อส่งข้อมูลและกำแพงที่มองไม่เห็น](https://www.openlawdatathailand.org/press/2025-12-31_blognone/5.jpg)

## บทที่ 6: บั๊กเส้นแบ่งเวลาและการผนึกกำลังแก้ปัญหา

#### เมื่อผมสวมหมวก QC แล้วเจอ "รอยร้าว"

ในขณะที่ Data Pipeline กำลังทำงานอย่างขยันขันแข็ง ผมในฐานะคนเขียนโค้ด ก็ต้องสวมหมวกอีกใบคือ **"QC (Quality Control)"** เพื่อตรวจสอบความเรียบร้อยของข้อมูลด้วยตัวเอง (เท่าที่พอจะทำไหว)

จากการนั่งไล่ดูไฟล์ JSON และชื่อไฟล์ PDF ที่ไหลเข้ามา ผมสังเกตเห็นความผิดปกติที่ชวนปวดหัว **"ทำไมประกาศของวันที่ 24 ถึงถูกบันทึกเป็นไฟล์วันที่ 23?"**

เมื่อเจาะลึกลงไปดูค่า Timestamp ที่ API ส่งมา ผมก็ตั้งสมมติฐานทันทีว่า นี่คือ **"Timezone Bug"**

* **สาเหตุ:** ระบบ API ส่งเวลามาเป็นมาตรฐานสากล (UTC) แต่เมื่อถูกแปลงเป็นวันที่เพื่อตั้งชื่อไฟล์ ระบบไม่ได้ชดเชยเวลาท้องถิ่นประเทศไทย (GMT+7)
* **ผลลัพธ์:** ประกาศที่ออกในช่วงเช้าตรู่ของไทย จึงถูกปัดเศษถอยหลังไปเป็น "เมื่อวาน" ทันที ทำให้ข้อมูลวันที่ผิดเพี้ยนไปจำนวนมาก


#### การติดต่อประสานงานที่น่าประทับใจ

แทนที่จะแค่เขียนโค้ดแก้ปลายเหตุ (Hack) ฝั่งเราเพียงอย่างเดียว ผมตัดสินใจติดต่อกลับไปยัง **"ทีม IT ของสำนักเลขาธิการคณะรัฐมนตรี (ดูแลราชกิจจาฯ)"** เพื่อแจ้งปัญหานี้

สิ่งที่เกิดขึ้นผิดจากภาพจำระบบราชการเดิมๆ ที่หลายคนอาจกังวล

* **การตอบรับ:** ทีม IT รับเรื่องทันที ไม่มีการโยนเรื่อง
* **การตรวจสอบ:** ตรวจสอบตามสมมติฐานที่เราแจ้งไป และยืนยันว่า *"ใช่ครับ เป็นเรื่อง Timezone จริงๆ"*
* **การแก้ไข:** และที่น่าประทับใจที่สุดคือ **"การแก้ไขที่รวดเร็วมาก"** ทีมงานฝั่งราชกิจจาฯ ปรับจูนระบบหลังบ้านให้ถูกต้องตามมาตรฐานทันที ทำให้ API ส่งค่าวันที่ที่ถูกต้องแม่นยำออกมา

เหตุการณ์นี้ทำให้ผมรู้สึกขอบคุณการ Support ของทางฝั่งราชกิจจาฯ เป็นอย่างมาก ที่ใส่ใจในความถูกต้องของข้อมูลและทำงานเป็นทีมเดียวกันกับเราจริงๆ


#### ปฏิบัติการเก็บกวาดบ้าน

เมื่อต้นน้ำ (API) แก้ไขแล้ว ปัญหายังเหลืออยู่ที่ปลายน้ำ (เครื่องของเรา) คือไฟล์เก่านับหมื่นนับแสนไฟล์ที่โหลดมาผิดวันก่อนหน้านี้

เราจึงกลับมาใช้แผน **"ผ่าตัดเฉพาะจุด"** เพื่อประหยัดทรัพยากร:

1. **ดึงข้อมูลเทียบ:** ผมเขียนสคริปต์ดึง "Index ที่ถูกต้อง" จาก API (ที่พี่เขาแก้แล้ว) มาเทียบกับไฟล์ในเครื่อง
2. **Smart Rename:** สั่งให้สคริปต์วิ่งไล่แก้ชื่อไฟล์และ Metadata ของไฟล์ที่มีปัญหา ให้กลับมาตรงตามปฏิทินไทย 100% โดยไม่ต้องโหลดไฟล์ PDF ใหม่ให้เปลือง Bandwidth ของภาครัฐ


#### บทสรุป: คุณภาพข้อมูลเกิดจากความร่วมมือ

บทนี้พิสูจน์ให้เห็นว่า **Open Law Data** ไม่ใช่แค่โครงการของฝ่ายใดฝ่ายหนึ่ง แต่มันคือการทำงานร่วมกัน

* ภาคประชาชนช่วย **"ตรวจสอบ"** (Audit)
* ภาครัฐช่วย **"สนับสนุนและแก้ไข"** (Support & Fix)

ผลลัพธ์ที่ได้คือ ฐานข้อมูลที่มีคุณภาพ ถูกต้อง และน่าเชื่อถือ พร้อมสำหรับการใช้งานจริงในที่สุด

![บั๊กเส้นแบ่งเวลาและการผนึกกำลังแก้ปัญหา](https://www.openlawdatathailand.org/press/2025-12-31_blognone/6.jpg)


## บทที่ 7: ปลดล็อกกำแพงภาษาด้วย AI และพันธมิตรใจดี

#### เมื่อ "รูปภาพ" ไม่ใช่ "ข้อมูล"

การที่เราดาวน์โหลดไฟล์ราชกิจจานุเบกษามาได้ 1.3 ล้านไฟล์ และแก้เรื่อง Timezone จนถูกต้องแล้ว ไม่ได้หมายความว่างานจบ เพราะไฟล์ส่วนใหญ่ (โดยเฉพาะไฟล์เก่าก่อนปี 2550) อยู่ในรูปแบบ PDF ที่เปิด Copy ข้อมูลออกมาได้ และแบบ "Scanned PDF"

ในสายตาของคอมพิวเตอร์ "Scanned PDF" ไฟล์เหล่านี้เป็นเพียง **"รูปภาพ"** (Images) เท่านั้น

* คอมพิวเตอร์ไม่รู้ว่าในภาพเขียนว่า "กฎกระทรวง" หรือ "พระราชบัญญัติ"
* เราไม่สามารถกด Ctrl+F เพื่อค้นหาคำว่า "ภาษี" หรือ "สิ่งแวดล้อม" ในไฟล์เหล่านี้ได้
* ถ้าเราไม่แปลงภาพให้เป็นตัวหนังสือ ข้อมูลเหล่านี้ก็เป็นเพียง **"Dead Data"** ที่ค้นหาไม่ได้

#### ความปราบเซียนของภาษาไทย

ทางออกของเรื่องนี้คือเทคโนโลยี **OCR (Optical Character Recognition)** หรือการให้ AI แกะตัวอักษรจากภาพ แต่สำหรับภาษาไทย นี่คือโจทย์ระดับ "Nightmare Mode"

1. **สระและวรรณยุกต์ซ้อนกัน (Complex Script):** ภาษาอังกฤษเขียนเรียงกันบรรทัดเดียว แต่ภาษาไทยมี 4 ระดับ (พยัญชนะ, สระบน, สระล่าง, วรรณยุกต์) เอกสารเก่าๆ มักเจอปัญหาวรรณยุกต์ลอย หรือหางอักษรขาดหาย
2. **ฟอนต์โบราณ (Legacy Fonts):** เอกสารราชการไทยยุคเก่าใช้พิมพ์ดีด หรือฟอนต์เฉพาะทาง (เช่น ตระกูล JS หรือ DS เก่าๆ) ที่ AI ของฝรั่งอย่าง Google Vision หรือ Tesseract มักจะอ่านผิดๆ ถูกๆ (เช่น อ่าน "ฎ" เป็น "ภ" หรือ "บ" เป็น "ข")

เราต้องการ OCR Engine ที่มีความเชี่ยวชาญภาษาไทยขั้นสูง... แต่ของแบบนี้มักมีราคาแพง และเรา "ไม่มีงบ"


#### แสงสว่างจาก Tech Partner ไทย: iApp Technology

ในขณะที่เรากำลังกลุ้มใจว่าจะทำอย่างไรกับกองภูเขาไฟล์ภาพเหล่านี้ เราก็ได้รับความอนุเคราะห์จาก **"iApp Technology"** บริษัท AI ชั้นนำของคนไทย

ทาง iApp ได้ทราบข่าวโครงการ Open Law Data และเห็นถึงความตั้งใจที่จะทำประโยชน์เพื่อสาธารณะ จึงยื่นมือเข้ามาสนับสนุนด้วยการ **"มอบสิทธิ์การใช้งาน OCR API ให้ฟรี"** สำหรับโครงการนี้

นี่คือจุดเปลี่ยนสำคัญ (Game Changer) เพราะทำให้โครงการได้เครื่องมือระดับ Enterprise Grade มาใช้ฟรีๆ คือการปลดล็อกอุปสรรคที่ใหญ่ที่สุดของเรา


#### การเดินทางไกลของข้อมูล (The Long Run Processing)

เมื่อได้อาวุธหนักมาแล้ว เราจึงเริ่มกระบวนการแปลงข้อมูลทันที

* **The Process:** Data Pipeline บน NAS จะค่อยๆ ส่ง PDF เอกสารเก่ายิงไปที่ API ของ iApp ทีละหน้าไฟล์เพื่อให้ AI แกะออกมาเป็น Text แล้วส่งกลับมาบันทึกเก็บไว้
* **The Strategy:** เนื่องจากข้อมูลมีปริมาณมหาศาล (หลายล้านไฟล์) เราจึงวางแผนการทำงานแบบ **"มาราธอน" **เราตั้งบอทให้ค่อยๆ Process ไปเรื่อยๆ อย่างสม่ำเสมอ ตลอด 24 ชั่วโมง วันละนิดวันละหน่อย


#### จาก "ภาพเก่า" สู่ "ความรู้ใหม่"

ณ วันนี้ กระบวนการ OCR ยังคงทำงานอย่างต่อเนื่อง จากใหม่ไปเก่า และจะยังคงรันต่อไปอีกยาวนานกว่าจะครบถ้วนสมบูรณ์ 100%

แต่ทุกวินาทีที่ผ่านไป เอกสารประวัติศาสตร์ของชาติกำลังถูก "ชุบชีวิต" ขึ้นมาใหม่ จากกระดาษเก่าๆ ที่ซูมดูยากๆ กลายเป็น **"Searchable Text"** ที่นักวิจัยสามารถค้นหาเจอในเสี้ยววินาที

ต้องขอขอบคุณ **iApp Technology** ที่เป็นลมใต้ปีก สนับสนุนเทคโนโลยีเบื้องหลังที่สำคัญที่สุดนี้ ทำให้เราพิสูจน์ได้ว่า **"คนไทยทำเทคโนโลยีภาษาไทยได้ดีที่สุด"** และเมื่อภาคเอกชนจับมือกับภาคประชาชน เราสามารถสร้าง Impact ที่ยิ่งใหญ่ได้โดยไม่ต้องรองบประมาณ


![ปลดล็อกกำแพงภาษาด้วย AI และพันธมิตรใจดี](https://www.openlawdatathailand.org/press/2025-12-31_blognone/7.jpg)


## บทที่ 8: พลังแห่งโครงสร้างและการเข้าถึงผ่าน CLI

#### โจทย์: ทำอย่างไรให้ Data 1.3 ล้านไฟล์ "ใช้ง่าย" เหมือนอยู่ในเครื่องตัวเอง?

เมื่อเรามีข้อมูลมหาศาลและหลากหลายรูปแบบ (PDF, ZIP, JSON, Metadata) โจทย์ที่ท้าทายที่สุดไม่ใช่ "การเก็บ" แต่คือ **"การหยิบใช้" (Retrieval)**

หากเรากองทุกอย่างรวมกัน นักพัฒนาที่อยากได้แค่ "ข้อมูลปี 2024" อาจต้องโหลดไฟล์ทั้งถังเพื่อมาคัดแยกเอง หรือถ้าระบบซับซ้อนเกินไป (เช่นต้องเขียน Code Python ยาวๆ เพื่อดึงข้อมูล) คนก็จะถอดใจ

เราจึงเลือกวิธีที่ **Simple but Powerful** นั่นคือการออกแบบ **"Directory Structure"** (โครงสร้างโฟลเดอร์) ให้เป็นระบบระเบียบที่สุด เพื่อให้สามารถใช้เครื่องมือมาตรฐานอย่าง **Hugging Face CLI** เข้าถึงข้อมูลแบบ "ผ่าตัด" (Surgical Download) ได้ทันทีโดยไม่ต้องพึ่งพา API ซับซ้อน


#### การออกแบบโครงสร้าง: แยกส่วนเพื่อมวลชน (The Separation of Concerns)

เราจัดระเบียบข้อมูลบน Cloud โดยแยกประเภทตามการใช้งานจริง ดังนี้:

1. **ocr/**: ขุมทรัพย์สำหรับสาย AI เก็บเฉพาะ Text ที่แกะออกมาแล้ว (JSON)
2. **meta/**: ขุมทรัพย์สำหรับสาย Data Analyst เก็บเฉพาะรายละเอียด (ชื่อเรื่อง, วันที่, เล่ม/ตอน)
3. **pdf/ (Hot Data):** ไฟล์ต้นฉบับย้อนหลัง 3 เดือน สำหรับคนที่ต้องการความสดใหม่
4. **zip/ (Cold Data):** ไฟล์ต้นฉบับอดีต (พ.ศ.2428 - ปัจจุบัน) บีบอัดรายเดือน เพื่อประหยัดพื้นที่


#### Magic Command: พลังของ Glob Pattern

ความเจ๋งของการจัดโครงสร้างแบบนี้ คือเราสามารถใช้ **Wildcard (`*`, `?`)** ในการสั่งโหลดข้อมูลได้เหมือนการจัดการไฟล์ในเครื่องตัวเอง

นี่คือ **Usage Instruction** ที่เราภูมิใจนำเสนอให้นักพัฒนา:


##### 1. สำหรับสาย AI / NLP

โจทย์ของคนกลุ่มนี้คือ "ขอ Text ไปเทรนโมเดล ไม่เอาไฟล์ขยะ" เราสามารถสั่ง CLI ให้ดึงเฉพาะโฟลเดอร์ `ocr` และ `meta` ของช่วงปีที่ต้องการได้เลย เช่น อยากได้ข้อมูลยุค 2020s (2020-2029) ก็แค่พิมพ์:


```
# ดึง Metadata ของยุค 2020s
hf download open-law-data-thailand/soc-ratchakitcha --repo-type dataset --include "meta/202?/*" --local-dir "downloads"
# ดึง OCR Text ของยุค 2020s
hf download open-law-data-thailand/soc-ratchakitcha --repo-type dataset --include "ocr/*/202?/*" --local-dir "downloads"
```


*สังเกตตัว `202?` นี่คือความยืดหยุ่นที่เรามอบให้ User สามารถกวาดข้อมูลทั้งทศวรรษได้ในบรรทัดเดียว*


##### 2. สำหรับสาย Data Analyst

โจทย์คือ "อยากวิเคราะห์เทรนด์กฎหมาย ไม่ต้องการเนื้อหายาวๆ" ท่านสามารถสั่งโหลดเฉพาะโฟลเดอร์ `meta` ทั้งหมดได้ ไฟล์จะเล็กมากและโหลดเสร็จในพริบตา:


```
hf download open-law-data-thailand/soc-ratchakitcha --repo-type dataset --include "meta/*/*" --local-dir "downloads"
```



##### 3. สำหรับสาย Archivist (นักเก็บข้อมูล)

โจทย์คือ "ต้องการไฟล์ต้นฉบับไว้อ้างอิง" เราใช้ระบบ **Hot/Cold Storage** เข้ามาจัดการ:



* **Hot Data (3 เดือนล่าสุด):** โหลดเป็นไฟล์ PDF รายฉบับ \
`hf download open-law-data-thailand/soc-ratchakitcha --repo-type dataset --include "pdf/*/*/*" --local-dir "downloads"`
* **Cold Data (ย้อนหลัง 100 ปี):** โหลดเป็นไฟล์ ZIP (โหลดง่าย ไม่รกเครื่อง) \
`hf download open-law-data-thailand/soc-ratchakitcha --repo-type dataset --include "zip/*/*" --local-dir "downloads"`


#### บทสรุป: ความอิสระของผู้ใช้งาน

แนวทางนี้ทำให้ **Open Law Data Thailand** ไม่ใช่แค่ Dataset ธรรมดา แต่เป็นเหมือน **File System บน Cloud** ที่นักพัฒนาสามารถเลือกหยิบจับส่วนประกอบไหนก็ได้ตามต้องการ

ไม่ต้องเขียนโค้ด Python ยุ่งยาก ไม่ต้องรอ API ตอบกลับ CLI ที่นักพัฒนาคุ้นเคย ข้อมูลกฎหมายไทยนับล้านหน้าก็พร้อมไปอยู่ในเครื่องของท่านทันที


![พลังแห่งโครงสร้างและการเข้าถึงผ่าน CLI](https://www.openlawdatathailand.org/press/2025-12-31_blognone/8.jpg)


## บทที่ 9: เว็บไซต์หน้าบ้านที่เรียบง่ายและการสนับสนุนจาก DGA

#### เมื่อข้อมูลพร้อม แต่คนยังไม่เห็น

หลังจากที่เราฝ่าฟันอุปสรรคทางเทคนิคมาสารพัด จนได้ Dataset ที่สะอาดและมีโครงสร้างสวยงามอยู่บน Hugging Face แล้ว ปัญหาต่อมาคือ **"การเข้าถึง"**

สำหรับ Developer การใช้ CLI หรือ Python Script อาจจะดูเท่และสะดวก แต่สำหรับท่าน สว., นักข่าว, นักวิจัยสังคม หรือประชาชนทั่วไป การจะให้มานั่งพิมพ์ Command Line จอดำๆ เพื่อเช็คว่า "วันนี้มีกฎหมายใหม่ออกมาไหม?" คงไม่ใช่เรื่องที่เหมาะสม

เราจำเป็นต้องมี **"หน้าบ้าน" (Frontend Website)** ที่ทำหน้าที่เป็นประตูบานใหญ่ เปิดต้อนรับทุกคนให้เข้ามาเห็นศักยภาพของข้อมูลชุดนี้ได้ง่ายที่สุด


#### สถาปัตยกรรม "ไฟล์เดียวรู้เรื่อง"

แทนที่จะสร้าง Backend ซับซ้อน หรือต้องยิง API ไปถาม Hugging Face ตลอดเวลา (ซึ่งอาจติด Rate Limit) เราเลือกใช้วิธีที่ **"กำปั้นทุบดินแต่ทรงประสิทธิภาพ"**



1. **The Trigger:** ทุกครั้งที่ Data Pipeline บน NAS ทำงานเสร็จสิ้น (ดูดข้อมูล -> แปลงไฟล์ -> อัปโหลด) ขั้นตอนสุดท้ายก่อนจบงาน คือการสร้างไฟล์สรุปสถานะ (Status File) ขึ้นมา 1 ไฟล์
2. **The Commit:** Pipeline จะทำการ **Commit & Push** ไฟล์สรุปนี้ขึ้นไปทับไฟล์เดิมที่ **Path เดิมเสมอ** บน Hugging Face
3. **The Fetch:** ตัวเว็บไซต์หน้าบ้าน มีหน้าที่เพียงแค่ดึง (Fetch) ไฟล์ Raw นี้มาแสดงผล

**ทำไมวิธีนี้ถึงดี?**

* **Real-time Visibility:** ทันทีที่ Pipeline ทำงานจบ เว็บไซต์จะแสดงข้อมูล "อัปเดตล่าสุด: เมื่อสักครู่นี้" ได้ทันที ผู้ใช้งานจะเห็นความสดใหม่ของข้อมูลเสมอ
* **SLA Tracking (อนาคต):** ด้วยโครงสร้างนี้ ในอนาคตเราสามารถแตกไฟล์ Status แยกตาม Data Source ได้ (เช่น ไฟล์สถานะของราชกิจจาฯ, ไฟล์สถานะของมติ ครม.) ทำให้เราทำ Dashboard ติดตาม **SLA (Service Level Agreement)** ได้เลยว่า แหล่งข้อมูลไหน "ตาย" หรือ "ไหลช้า" ผิดปกติ โดยไม่ต้องเขียนระบบ Monitoring แพงๆ


#### บ้านที่อบอุ่นจาก DGA

สำหรับการวางโฮสต์เว็บไซต์ (Hosting) เราได้รับความอนุเคราะห์จาก **สำนักงานพัฒนารัฐบาลดิจิทัล (องค์การมหาชน) หรือ สพร. (DGA)**

ทาง DGA เล็งเห็นประโยชน์ของโครงการนี้ จึงอนุญาตให้เรานำ Source Code ของเว็บไซต์ ขึ้นไปวางไว้บน **GitHub Organization ของ DGA-Thailand** และเปิดใช้งาน **GitHub Pages** เพื่อใช้เป็น Web Hosting ให้ฟรี

การสนับสนุนจาก DGA มีความหมายมากกว่าแค่เรื่องเทคนิค:


1. **ความน่าเชื่อถือ (Credibility):** การที่เว็บไซต์รันอยู่ภายใต้ Domain ของ DGA หรือ GitHub ของหน่วยงานรัฐ ทำให้ประชาชนมั่นใจได้ว่า นี่ไม่ใช่เว็บเถื่อน แต่เป็นโครงการที่มีมาตรฐาน
2. **มาตรฐาน Open Source:** การเปิดเผย Code บน GitHub ของ DGA เป็นการประกาศจุดยืนเรื่องความโปร่งใส ใครๆ ก็เข้ามาตรวจสอบ (Audit) หรือช่วยกันพัฒนา (Contribute) ต่อได้
3. **Zero Cost:** เราไม่ต้องเสียเงินค่า Server ทำให้โครงการนี้ยั่งยืน (Sustainable) ไม่มีต้องมีค่าใช้จ่ายสนับสนุนในอนาคต (เหลือค่า Domain ที่ยังต้องจ่าย)


#### บทสรุป: ประตูสู่โลกกว้าง

วันนี้ [https://www.openlawdatathailand.org](https://www.openlawdatathailand.org) ที่รันอยู่บน GitHub Pages ของ DGA ไม่ได้เป็นเพียงหน้าเว็บธรรมดา แต่มันคือ **Hub** ที่รวบรวมทั้ง "ข้อมูล" (Data) และ "สถานะการทำงาน" (Health Check) ไว้ในที่เดียว

ต้องขอขอบคุณ **DGA (สพร.)** ที่เป็นลมใต้ปีก สนับสนุนพื้นที่และโอกาสให้ภาคประชาชน (Civic Tech) ได้ทำงานร่วมกับภาครัฐอย่างใกล้ชิด เพื่อสร้าง Digital Service ที่โปร่งใสและตรวจสอบได้เพื่อคนไทยทุกคน


![เว็บไซต์หน้าบ้านที่เรียบง่ายและการสนับสนุนจาก DGA](https://www.openlawdatathailand.org/press/2025-12-31_blognone/9.jpg)


## บทที่ 10: วันปล่อยของ ปรากฏการณ์ไวรัล และรุ่งอรุณใหม่ของข้อมูลไทย

#### กดปุ่ม Start: การเริ่มต้นที่เงียบเชียบ

หลังจากซุ่มทำ Data Pipeline, รัน OCR ข้ามวันข้ามคืน, และแก้บั๊กสารพัดร่วมกับพี่ๆ ทีม IT ภาครัฐ ในที่สุดวันที่เรารอคอยก็มาถึง

เราไม่ได้จัดงานแถลงข่าวใหญ่โตที่โรงแรมหรู ไม่มีการตัดริบบิ้น มีเพียงโพสต์ Facebook ธรรมดาๆ หนึ่งโพสต์ ที่ประกาศบอกเพื่อนฝูงและ Community นักพัฒนาว่า:


[**"ปล่อยของครับ! 🚀 Open Data ราชกิจจานุเบกษาตั้งแต่ปี 2428 พร้อมโหลดแล้ว! (Beta)"**](https://www.facebook.com/kriangkrai.chaonithi/posts/pfbid0VPNu3eF5EnyVwkS6qLVkmikXtecMcg55X4qZVzr1euaMZ6w3HQjFbWw8hH89osbEl)

ในวินาทีแรก ผมคาดหวังเพียงแค่ว่า ขอให้มีคนเห็นสัก 50 คน หรือมีนักพัฒนาสัก 2-3 คนลองโหลดไปใช้ เราก็ดีใจแล้ว เพราะเรารู้ดีว่าเรื่อง "กฎหมาย" และ "Data Engineering" เป็นเรื่องเฉพาะกลุ่มมากๆ


#### ปรากฏการณ์ไวรัล: เมื่อยอดแชร์แซงยอดไลก์

แต่สิ่งที่เกิดขึ้นจริง กลับถล่มทลายเกินคาด ภายในเวลาไม่ถึง 24 ชั่วโมง โพสต์นั้นกระจายออกไปไกลมาก


* **ยอดแชร์ (Share) สูงกว่ายอดไลก์ (Like) กว่า 2 เท่า** (906 Shares vs 410 Likes)

ในทางสถิติ Social Media ตัวเลขนี้มีความหมายลึกซึ้ง มันบ่งบอกว่าคนไม่ได้แค่ "ชอบ" แล้วผ่านไป แต่พวกเขารู้สึกว่าสิ่งนี้ **"มีประโยชน์"** และ **"ต้องเก็บไว้"**


* คอมเมนต์หลั่งไหลเข้ามา ทั้งจาก Dev รุ่นใหญ่, Startup, ไปจนถึงอาจารย์มหาวิทยาลัย
* คีย์เวิร์ดอย่าง *"LawGPT"*, *"ในที่สุดก็มีวันนี้"*, *"ขอบคุณที่ทำ"* สะท้อนให้เห็นว่า **สังคมไทย "หิวกระหาย" ข้อมูลคุณภาพมานานแค่ไหน**

![วันปล่อยของ](https://www.openlawdatathailand.org/press/2025-12-31_blognone/10.jpg)

## ภารกิจยังไม่จบ: เดินหน้าสู่คณะทำงานชุดที่ 2

ความสำเร็จของ Dataset ชุดแรก ไม่ใช่เส้นชัย แต่เป็นเพียง **"Proof of Concept"** ที่พิสูจน์แล้วว่า โมเดลความร่วมมือระหว่างภาคประชาชนและภาครัฐนั้นเป็นไปได้และมีประสิทธิภาพ

ขณะนี้ ทีมงานกำลังเตรียมการจัดตั้ง **"คณะทำงานชุดที่ 2"** เพื่อขยายผลความสำเร็จนี้ออกไป


* **เป้าหมาย:** เราจะดำเนินการติดต่อและประสานงานกับหน่วยงานภาครัฐอื่นๆ เพิ่มเติม
* **ภารกิจ:** เฟ้นหาข้อมูลที่มีสถานะเป็น "ข้อมูลสาธารณะ" (Public Information) อยู่แล้ว แต่ยังติดขัดเรื่องรูปแบบการจัดเก็บหรือการเข้าถึง เพื่อนำมาเข้าสู่กระบวนการ Data Pipeline เผยแพร่ให้แก่นักพัฒนาและประชาชน

เราเชื่อว่ายังมีขุมทรัพย์ข้อมูลอีกมหาศาลที่รอการปลดปล่อย และเราพร้อมที่จะเข้าไปอำนวยความสะดวกให้หน่วยงานเหล่านั้น เพื่อเปลี่ยนข้อมูลที่หลับใหล ให้กลายเป็น Open Data ที่ใช้งานได้จริง

## Call to Action: มาร่วมสร้างระบบนิเวศไปด้วยกัน

โครงการ **Open Law Data Thailand** เดินมาถึงจุดนี้ได้ด้วยพลังอาสา แต่จะเดินต่อไปให้ไกลกว่าเดิม ต้องอาศัยพลังของทุกคน ผมจึงอยากขอเชิญชวน 3 สิ่งครับ:


##### 1. ขอแรง Contributors (Join Our Discord)

เราได้ตั้ง **Discord Community** ขึ้นมา เพื่อเป็นแหล่งรวมตัวของคนบ้าพลัง


* ใครเก่ง Python มาช่วยแก้ Pipeline
* ใครเก่ง Data มาช่วยทำ Clean Data
* ใครเป็น User มาช่วยแจ้งบั๊ก
* *มาร่วมเป็นส่วนหนึ่งของประวัติศาสตร์ Open Source ไทยกันครับ*
* **Join our Community on Discord:** [https://openlawdatathailand.org/discord/](https://openlawdatathailand.org/discord/)


##### 2. อย่าโหลดไปดอง... จงสร้างของจริง! (Build Real Use Cases)

ข้อมูลที่เราแจกไป จะไม่มีค่าเลยถ้ามันนอนนิ่งอยู่ในฮาร์ดดิสก์ ผมอยากเห็น **Innovation** เกิดขึ้นจริง:


* สร้าง **AI Legal Advisor:** แชทบอทที่ตอบปัญหากฎหมายชาวบ้านได้แม่นยำ
* สร้าง **Alert System:** แจ้งเตือนผู้ประกอบการเมื่อมีกฎหมายใหม่ที่กระทบธุรกิจ
* สร้าง **Regulatory Guillotine** ช่วยภาครัฐวิเคราะห์หากฎหมายที่ล้าสมัย ซ้ำซ้อน หรือขัดแย้งกันเอง
* มาช่วยกันทำให้เห็นว่า **Open Data กินได้และใช้ประโยชน์ได้จริง**


##### 3. ความหวังถึง "มาตรฐานภาครัฐ" (The Standard Protocol)

สุดท้ายนี้ ความฝันสูงสุดของผมและทีมงานทุกคน ไม่ใช่การทำโครงการนี้ไปตลอดกาล แต่อยากให้โครงการนี้เป็น **โครงการต้นแบบ** ที่พิสูจน์ให้ภาครัฐเห็นว่า **"ทำเถอะ มันทำได้จริง และมันคุ้มค่า"**

ผมหวังว่าในอนาคต เราจะไม่ต้องมานั่งดูดข้อมูล หรือรัน OCR กันเองอีก แต่ภาครัฐจะกำหนด **Standard Protocol** ในการส่งมอบข้อมูล

* กฎหมายทุกฉบับที่คลอดออกมา ควรเป็น Machine-Readable ตั้งแต่วันแรก
* ควรมี API กลางของชาติที่นักพัฒนาเชื่อมต่อได้ทันที นำไปใช้งานต่อยอดได้โดยตรง
* เผยแพร่ข้อมูลใน Platform ที่นักพัฒนาสามารถนำไปใช้งานต่อได้โดยสะดวก ไม่เปลืองงบประมาณของภาครัฐจนเกินจำเป็น

เมื่อถึงวันนั้น... บทบาทของ Open Law Data Thailand ในฐานะ "คนกลาง" อาจจะหมดไป แต่เราจะดีใจที่สุด เพราะนั่นหมายความว่า **"รุ่งอรุณแห่งรัฐบาลดิจิทัลและข้อมูลเปิดของประเทศไทย ได้เกิดขึ้นอย่างสมบูรณ์แล้ว"**

![มาร่วมสร้างระบบนิเวศไปด้วยกัน](https://www.openlawdatathailand.org/press/2025-12-31_blognone/11.jpg)

## บทส่งท้าย: บันทึกคำขอบคุณ

ความสำเร็จของโครงการ **Open Law Data Thailand** ในวันนี้ ไม่ได้เกิดขึ้นจากคนเพียงกลุ่มเดียว แต่เกิดจากการผนึกกำลังของภาครัฐ ภาคเอกชน และภาคประชาสังคม ที่มีวิสัยทัศน์ร่วมกันในการขับเคลื่อนประเทศไทยด้วยข้อมูล

ในนามของคณะทำงานและหัวหน้าทีมพัฒนา ผมขอขอบพระคุณทุกท่านและทุกหน่วยงานที่ให้การสนับสนุน ดังนี้:


#### ผู้ริเริ่มและขับเคลื่อนเชิงนโยบาย

* **คณะกรรมาธิการการพาณิชย์และการอุตสาหกรรม วุฒิสภา**
* **คณะทํางาน Open Law Data**
    * นำโดย **ท่านสมาชิกวุฒิสภา ตวงคุณ ทรงธรรมวัฒน์** ผู้ริเริ่มโครงการ ที่เล็งเห็นความสำคัญของโครงสร้างพื้นฐานทางข้อมูล และผลักดันให้เกิดความร่วมมืออย่างเป็นรูปธรรม


#### ผู้สนับสนุนข้อมูลและโครงสร้างพื้นฐานภาครัฐ

* **สำนักเลขาธิการคณะรัฐมนตรี (สลค.)**
    * ผู้สนับสนุนข้อมูล **"ราชกิจจานุเบกษา"** และให้ความร่วมมือในการปรับปรุงระบบเชื่อมต่อ เพื่อให้ข้อมูลไหลเวียนสู่สาธารณะได้อย่างถูกต้องและมีประสิทธิภาพ
* **สำนักงานพัฒนารัฐบาลดิจิทัล (องค์การมหาชน) (สพร. หรือ DGA)**
    * ผู้สนับสนุน **โครงสร้างพื้นฐานดิจิทัล (Digital Infrastructure)** และพื้นที่สำหรับเว็บไซต์ เพื่อให้ประชาชนเข้าถึงข้อมูลได้อย่างมั่นคงและยั่งยืน


#### ผู้สนับสนุนทรัพยากรและฮาร์ดแวร์

* **สภาอุตสาหกรรมจังหวัดสมุทรสงคราม**
    * ผู้สนับสนุนเครื่อง **Synology NAS** ซึ่งเปรียบเสมือนหัวใจสำคัญในการเก็บข้อมูลและประมวลผล Data Pipeline ตลอด 24 ชั่วโมง


#### ผู้สนับสนุนเทคโนโลยีและความเชี่ยวชาญ

* **พี่แก๊บ UpPass (ผู้เขียน)**
    * **"สนับสนุนเทคโนโลยีและองค์ความรู้ด้าน Data Engineering"**
    * ขอบคุณทีมงานที่นำประสบการณ์จากการจัดการข้อมูลขนาดใหญ่ ช่วยพัฒนาแบบระบบ Data Pipeline ให้ทำงานได้อย่างอัตโนมัติ
* **พี่ซี iApp Technology**
    * **"สนับสนุนเครื่องมือการทำ OCR"**
    * ขอบคุณเทคโนโลยีปัญญาประดิษฐ์ (AI) ฝีมือคนไทยที่มีความแม่นยำสูง ช่วยปลดล็อกกำแพงภาษา เปลี่ยนภาพเอกสารนับล้านหน้าให้กลายเป็นข้อมูลที่สืบค้นได้จริง


#### ผู้ประสานงานและเชื่อมโยงเครือข่าย



* **พี่แม็กซ์ StockRadars**
    * ผู้เป็น "สะพาน" เชื่อมโยงบุคลากรและทรัพยากรทางด้านเทคโนโลยี ทำให้ผมและทีมงานได้รับการสนับสนุนที่จำเป็น และทำให้โครงการนี้เกิดขึ้นได้จริง

**ด้วยความขอบคุณยิ่ง** **ทีมงาน Open Law Data Thailand**


![ขอขอบคุณ NAS ที่มุมห้อง](https://www.openlawdatathailand.org/press/2025-12-31_blognone/12.jpg)


## The Upcoming Event

#### Open Law Data Hackathon 2026

เพื่อให้ข้อมูลที่เปิดออกมาเกิดประโยชน์สูงสุด ทางคณะทำงานมีแผนจะจัดงาน **"Open Law Data Hackathon"** ครั้งแรก คาดว่าน่าจะเป็นช่วง **ต้น-กลาง ปี 2026**


* **Challenge:** ชวนนักพัฒนา นิติกร และ Data Scientist มาร่วมทีมกันสร้างนวัตกรรมจากข้อมูลกฎหมาย เช่น AI Lawyer Assistant, Regulatory Guillotine, Corruption Radar และอื่นๆ ตามแต่จินตนาการพวกท่านจะนึกถึง

แล้วเตรียมพบกัน ติดตามข่าวสารผ่านทาง Discord

**👉 Join our Community on Discord:** [https://openlawdatathailand.org/discord/](https://openlawdatathailand.org/discord/)

---

#### Special Mention: เบื้องหลังที่ไม่ลับกับ "Gemini" และก้าวต่อไปของ Dev ไทย

สุดท้ายที่ขาดไม่ได้ คือการให้เครดิตกับเพื่อนคู่คิดที่เป็น AI อย่าง **"Gemini"**

ความจริงที่ผมอยากแชร์กับเพื่อนนักพัฒนาทุกคนคือ **Code เกือบทั้งหมดในโปรเจกต์นี้ รวมถึงบทความที่คุณกำลังอ่านอยู่นี้ ถูกเขียนและเรียบเรียงขึ้นโดยมี AI มากกว่า 98% (แต่ผมยังรีวิวและอ่านทุกตัวอักษรนะ 😛)**

นี่คือ **Proof of Concept (PoC)** ที่ชัดเจนที่สุดที่ผมอยากพิสูจน์ให้เห็นว่า:

**การใช้ AI พัฒนาโครงการระดับประเทศ (National Scale) ด้วยทีมงานขนาดเล็กที่ "มีนักพัฒนาเพียงคนเดียว" มันเป็นไปได้จริงแล้ว**

ผมอยากฝากข้อความนี้ถึงนักพัฒนาไทยทุกคน: โลกเปลี่ยนไปแล้ว การเขียนโค้ดแบบเดิมอาจไม่เพียงพออีกต่อไป ผมอยากเชิญชวนให้พวกเรา **"คุณต้องปรับตัว ยกระดับวิธีการทำงาน และเรียนรู้ที่จะใช้งาน AI อย่างถูกต้องและถูกวิธี"**

**เมื่อเราผสานความเข้าใจในปัญหา (Domain Expertise) เข้ากับพลังความเร็วของ AI เราจะสามารถสร้าง Impact ได้มหาศาลเกินกว่าที่มนุษย์คนเดียวจะทำได้ในอดีต** และนี่คือกุญแจสำคัญที่จะทำให้นักพัฒนาชาวไทย ก้าวทันโลกและยืนหยัดในการแข่งขันระดับสากลได้อย่างเต็มภาคภูมิครับ


> โครงการ Open Law Data เป็น "สมบัติของทุกคน"...  นี่คือพันธกิจที่เราจะร่วมกันผลักดันให้เกิดบรรทัดฐานใหม่ เพื่อให้ข้อมูลของภาครัฐเป็นสิ่งที่ทุกคนเข้าถึงได้ง่าย และนำไปต่อยอดให้เกิดประโยชน์ได้จริง